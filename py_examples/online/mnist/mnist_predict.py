from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
import numpy as np
import json
from json import JSONEncoder
# import onnxruntime as onnxrt

class NumpyArrayEncoder(JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return JSONEncoder.default(self, obj)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

def save_onnx_model(model, model_name):
    input_names = [ "input" ]
    output_names = [ "output" ]

    x = torch.randn(1, 1, 28, 28, requires_grad=True)

    torch.onnx.export(model,
                x,
                f"models/{model_name}.onnx",
                verbose=False,
                input_names=input_names,
                output_names=output_names,
                export_params=True,
                )

# def onnx_predict(model_name, x):
#     onnx_session= onnxrt.InferenceSession(f"models/{model_name}.onnx")
#     def to_numpy(tensor):
#         return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()
#     onnx_inputs= {onnx_session.get_inputs()[0].name:to_numpy(x)}
#     onnx_output = onnx_session.run(None, onnx_inputs)
#     img_label = onnx_output[0]
#     return img_label

            
def mnist_predict(input_json):
    # Training settings
    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
    parser.add_argument('--use-onnx', action='store_true',default=False,
                        help='Use the onnx model for inference')
    args = parser.parse_args()

    device = torch.device("cpu")

    model_name = 'mnist_cnn'

    # load data from input json
    input_data= input_json['input_data']
    data1D = input_data[0]['values'][0][0]
    data4D = np.reshape(data1D, (1, 1, 28, 28))
    data4D = np.float32(data4D)

    x_np = torch.from_numpy(data4D)
    x_np = x_np.to(device)
    
    if args.use_onnx:
        #output1D = onnx_predict(model_name,x_np)
        output1D = [1]
        print('used onnx')
    else:
        model = Net().to(device)
        model.load_state_dict(torch.load(f"models/{model_name}.pt"))
        # model.load_state_dict(torch.load("/mnts/cpd-file-1tb/models/mnist_cnn.pt"))
        model.eval()
        output = model(x_np)
        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability

        output1D = output.cpu().detach().numpy().ravel()
        pred = pred.cpu().numpy().ravel()
        print('used pytorch')

    # Serialization
    numpyData = {
        "fields": [
            "prediction_classes",
            "probability"
        ],
        "values": [[pred, output1D]]
    }
    output_json = json.dumps(numpyData, cls=NumpyArrayEncoder)  # use dump() to write array into file
    print("Printing JSON serialized NumPy array")
    print(output_json)
    
    return output_json


def test_predict():
    # Training settings
    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
    parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                        help='input batch size for training (default: 64)')
    parser.add_argument('--test-batch-size', type=int, default=1, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=2, metavar='N',
                        help='number of epochs to train (default: 2)')
    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',
                        help='learning rate (default: 1.0)')
    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
                        help='Learning rate step gamma (default: 0.7)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--no-mps', action='store_true', default=False,
                        help='disables macOS GPU training')
    parser.add_argument('--dry-run', action='store_true', default=False,
                        help='quickly check a single pass')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                        help='how many batches to wait before logging training status')
    parser.add_argument('--save-model', action='store_true', default=True,
                        help='For Saving the current Model')
    args = parser.parse_args()
    use_cuda = not args.no_cuda and torch.cuda.is_available()
    use_mps = not args.no_mps and torch.backends.mps.is_available()

    torch.manual_seed(args.seed)

    if use_cuda:
        device = torch.device("cuda")
    elif use_mps:
        device = torch.device("mps")
    else:
        device = torch.device("cpu")

    train_kwargs = {'batch_size': args.batch_size}
    test_kwargs = {'batch_size': args.test_batch_size}
    if use_cuda:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True}
        train_kwargs.update(cuda_kwargs)
        test_kwargs.update(cuda_kwargs)

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    dataset2 = datasets.MNIST('../../data', train=False,
                              transform=transform)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)

    model_name = 'mnist_cnn'
    model = Net().to(device)
    model.load_state_dict(torch.load(f"models/{model_name}.pt"))
    # model.load_state_dict(torch.load("/mnts/cpd-file-1tb/models/mnist_cnn.pt"))
    model.eval()

    save_onnx_model(model, model_name)

    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            #print(data)
            #print(target)

            data, target = data.to(device), target.to(device)

            data4D = data.cpu().numpy()
            data1D = data4D.ravel()

            # an example input
            # {"input_data": [{
            #     "fields": ["AGE", "SEXE"],
            #     "values": [
            #         [33, "F"],
            #         [59, "F"],
            #         [28, "M"]
            #     ]
            # }]}
            # Serialization
            numpyData = {"input_data": [{
                "fields": ["image"],
                "values": [
                    [[data1D]]
                ]
            }]}
            encodedNumpyData = json.dumps(numpyData, cls=NumpyArrayEncoder)  # use dump() to write array into file
            print("Printing JSON serialized NumPy array")
            print(encodedNumpyData)

            data4D = np.reshape(data1D,(1,1,28,28))

            #np.savetxt('my_file.txt', data.cpu().numpy())
            x_np = torch.from_numpy(data4D)
            x_np = x_np.to(device)
            output = model(x_np)
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability

            output1D = output.cpu().numpy().ravel()
            pred = pred.cpu().numpy().ravel()

            # Serialization
            #numpyData = {"values": [[output1D]]}
            numpyData = {
                "fields": [
                        "prediction_classes",
                        "probability"
                      ],
                "predictions": [
                    {
                        "values": [
                            pred,
                            [output1D]
                        ]
                    }
                ]
            }

            encodedNumpyData = json.dumps(numpyData, cls=NumpyArrayEncoder)  # use dump() to write array into file
            print("Printing JSON serialized NumPy array")
            print(encodedNumpyData)

            # output_json = {
            #     "fields": [
            #         "prediction_classes",
            #         "probability"
            #     ],
            #     "values": [
            #         [
            #             7,
            #             [
            #                 0.9999523162841797,
            #                 8.347302582478733e-08
            #             ]
            #         ],
            #         [
            #             2,
            #             [
            #                 8.570060003876279e-07,
            #                 0.9999991655349731
            #             ]
            #         ]
            #     ]
            # }



            val = input("Enter your value: ")
            print(val)


if __name__ == '__main__':
    input_json = {"input_data": [{"fields": ["image"], "values": [[[[-0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 0.6449587345123291, 1.930510401725769, 1.5995763540267944, 1.4977505207061768, 0.33948105573654175, 0.034003473818302155, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 2.4014549255371094, 2.808758497238159, 2.808758497238159, 2.808758497238159, 2.808758497238159, 2.643291473388672, 2.095977306365967, 2.095977306365967, 2.095977306365967, 2.095977306365967, 2.095977306365967, 2.095977306365967, 2.095977306365967, 2.095977306365967, 1.7395868301391602, 0.23765520751476288, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 0.4285787343978882, 1.0268056392669678, 0.4922199249267578, 1.0268056392669678, 1.6504892110824585, 2.4650962352752686, 2.808758497238159, 2.4396398067474365, 2.808758497238159, 2.808758497238159, 2.808758497238159, 2.757845640182495, 2.4905526638031006, 2.808758497238159, 2.808758497238159, 1.3577399253845215, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.20783297717571259, 0.41585052013397217, -0.2460176944732666, 0.4285787343978882, 0.4285787343978882, 0.4285787343978882, 0.32675284147262573, -0.15692004561424255, 2.5796501636505127, 2.808758497238159, 0.9249798059463501, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 0.6322304606437683, 2.796030282974243, 2.235987901687622, -0.19510474801063538, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.14419181644916534, 2.5414655208587646, 2.821486711502075, 0.6322304606437683, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 1.2177293300628662, 2.808758497238159, 2.605106830596924, 0.13582934439182281, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 0.32675284147262573, 2.7451171875, 2.808758497238159, 0.36493754386901855, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 1.2686423063278198, 2.808758497238159, 1.955966830253601, -0.36057180166244507, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.30965885519981384, 2.185075044631958, 2.732388973236084, 0.31402459740638733, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 1.179544448852539, 2.808758497238159, 1.8923256397247314, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 0.5304046273231506, 2.770573854446411, 2.630563259124756, 0.3012963533401489, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.18237650394439697, 2.3887267112731934, 2.808758497238159, 1.688673973083496, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.3860282599925995, 2.159618616104126, 2.808758497238159, 2.3632702827453613, 0.021275240927934647, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 0.05945993959903717, 2.808758497238159, 2.808758497238159, 0.5558610558509827, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.029637714847922325, 2.4269113540649414, 2.808758497238159, 1.0395338535308838, -0.4114847183227539, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 1.2686423063278198, 2.808758497238159, 2.808758497238159, 0.23765520751476288, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 0.35220929980278015, 2.656019687652588, 2.808758497238159, 2.808758497238159, 0.23765520751476288, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 1.1159032583236694, 2.808758497238159, 2.808758497238159, 2.3632702827453613, 0.08491640537977219, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, 1.1159032583236694, 2.808758497238159, 2.21053147315979, -0.19510474801063538, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923, -0.4242129623889923]]]]}]}
    output_json = mnist_predict(input_json)

    #test_predict()